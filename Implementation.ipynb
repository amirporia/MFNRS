{"cells":[{"cell_type":"markdown","metadata":{},"source":["Data Preprocessing"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T06:24:10.587609Z","iopub.status.busy":"2024-01-27T06:24:10.587126Z","iopub.status.idle":"2024-01-27T06:24:15.030817Z","shell.execute_reply":"2024-01-27T06:24:15.029625Z","shell.execute_reply.started":"2024-01-27T06:24:10.587571Z"},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.utils import shuffle\n","\n","def map_interest_rating(value): # re-evaluate rank to level of relevance\n","    if value == 0:\n","        return 0\n","    elif 0 < value <= 1/3:\n","        return 1\n","    elif 1/3 < value <= 2/3:\n","        return 2\n","    elif 2/3 < value <= 1:\n","        return 3\n","    else:\n","        return value\n","\n","class NFC_Data(object): # class for data roganization\n","  def __init__(self, args, ratings):\n","    self.ratings = ratings\n","    self.num_batchSize_test_valid = args[\"num_batchSize_test_valid\"] # batch size of test and validation data -- removed\n","    self.batch_size = args[\"batch_size\"]\n","    self.preprocess_ratings = self._reindex(self.ratings) # re-index users and items\n","    self.user_pool = set(self.ratings['User_ID'].unique())\n","    self.item_pool = set(self.ratings['Group_ID'].unique())\n","    self.train_ratings, self.test_ratings, self.validation_ratings = self.split_data(self.preprocess_ratings) # splitting data to validation and test and train dataSet\n","    random.seed(args[\"seed\"])\n","\n","  def _reindex(self, ratings):  # re-index users and items by unique id\n","    user2id = {w: i for i, w in enumerate(ratings['User_ID'].unique())}\n","    item2id = {w: i for i, w in enumerate(ratings['Group_ID'].unique())}\n","    ratings['User_ID'] = ratings['User_ID'].map(user2id)\n","    ratings['Group_ID'] = ratings['Group_ID'].map(item2id)\n","    return ratings\n","\n","  def split_data(self, ratings):\n","    # split data set to two group ( rating between 10 and 20 that estimate 20% of total dataSet and rest of dataSet )        \n","    result = (ratings[(ratings['User_ID'].groupby(ratings['User_ID']).transform('size') >= 10) & (ratings['User_ID'].groupby(ratings['User_ID']).transform('size') <= 20)])\n","    \n","    # Find unique User_IDs\n","    unique_user_ids = result['User_ID'].unique()\n","\n","    # Shuffle the unique User_IDs to get randomness\n","    np.random.shuffle(unique_user_ids)\n","\n","    # Create two DataFrames based on the split User_IDs\n","    validation_data = result[result['User_ID'].isin(unique_user_ids[:(len(unique_user_ids) // 2)])]\n","\n","    test_data = result[result['User_ID'].isin(unique_user_ids[(len(unique_user_ids) // 2):])]\n","    train_data = (ratings.subtract(validation_data, fill_value=0)).subtract(test_data, fill_value=0)\n","    \n","    # re-evaluate rating of test and validation Set because of measurement metrics\n","    test_data.loc[:, 'Interest_Rate'] = test_data['Interest_Rate'].apply(map_interest_rating)\n","    validation_data.loc[:, 'Interest_Rate'] = validation_data['Interest_Rate'].apply(map_interest_rating)\n","    \n","    return (\n","        train_data[['User_ID', 'Group_ID', 'Interest_Rate']],\n","        test_data[['User_ID', 'Group_ID', 'Interest_Rate']],\n","        validation_data[['User_ID', 'Group_ID', 'Interest_Rate']]\n","    )\n","\n","  def get_train_instance(self):\n","    users, items, ratings= [], [], []\n","    for row in self.train_ratings.itertuples():\n","      users.append(int(row.User_ID))\n","      items.append(int(row.Group_ID))\n","      ratings.append(float(row.Interest_Rate))\n","    dataset = Rating_Datset(\n","        user_list=users,\n","        item_list=items,\n","        rating_list=ratings\n","    )\n","    return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n","  \n","  def get_test_instance(self):\n","    return self.test_ratings\n","\n","  def get_validation_instance(self):\n","    return self.validation_ratings\n","\n","class Rating_Datset(torch.utils.data.Dataset):\n","\tdef __init__(self, user_list, item_list, rating_list):\n","\t\tsuper(Rating_Datset, self).__init__()\n","\t\tself.user_list = user_list\n","\t\tself.item_list = item_list\n","\t\tself.rating_list = rating_list\n","\n","\tdef __len__(self):\n","\t\treturn len(self.user_list)\n","\n","\tdef __getitem__(self, idx):\n","\t\tuser = self.user_list[idx]\n","\t\titem = self.item_list[idx]\n","\t\trating = self.rating_list[idx]\n","\t\t\n","\t\treturn (\n","\t\t\ttorch.tensor(user, dtype=torch.long),\n","\t\t\ttorch.tensor(item, dtype=torch.long),\n","\t\t\ttorch.tensor(rating, dtype=torch.float)\n","\t\t\t)"]},{"cell_type":"markdown","metadata":{},"source":["Neural Network Matrix factorization Collaborative Filtering Model Building"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T06:24:15.034043Z","iopub.status.busy":"2024-01-27T06:24:15.033374Z","iopub.status.idle":"2024-01-27T06:24:15.068215Z","shell.execute_reply":"2024-01-27T06:24:15.066969Z","shell.execute_reply.started":"2024-01-27T06:24:15.034000Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","# Custom RMSE Loss Function\n","class RMSELoss(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    \n","    def forward(self, y_pred, y_true):\n","        criterion = torch.nn.MSELoss()\n","        loss = torch.sqrt(criterion(y_pred, y_true))\n","        return loss\n","\n","class Generalized_Matrix_Factorization(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(Generalized_Matrix_Factorization, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num = args[\"factor_num\"]\n","\n","        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n","        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n","\n","        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n","        self.logistic = nn.Sigmoid()\n","\n","    def _forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        element_product = torch.mul(user_embedding, item_embedding)\n","        logits = self.affine_output(element_product)\n","        rating = self.logistic(logits)\n","        return rating\n","\n","    def init_weight(self):\n","        pass\n","\n","class Multi_Layer_Perceptron(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(Multi_Layer_Perceptron, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num = args[\"factor_num\"]\n","        self.layers = args[\"layers\"]\n","\n","        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n","        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n","\n","        self.fc_layers = nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n","            self.fc_layers.append(nn.Linear(in_size, out_size))\n","\n","        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n","        self.logistic = nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            vector = self.fc_layers[idx](vector)\n","            vector = nn.ReLU()(vector)\n","            # vector = nn.BatchNorm1d()(vector)\n","            # vector = nn.Dropout(p=0.5)(vector)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating\n","\n","    def init_weight(self):\n","        pass\n","\n","\n","\n","class NeuMF(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(NeuMF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num_mf = args[\"factor_num\"]\n","        self.factor_num_mlp =  int(args[\"layers\"][0]/2)\n","        self.layers = args[\"layers\"]\n","        self.dropout = args[\"dropout\"]\n","\n","        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n","        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n","\n","        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n","        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n","\n","        self.fc_layers = nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(args[\"layers\"][:-1], args[\"layers\"][1:])):\n","            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n","            self.fc_layers.append(nn.ReLU())\n","\n","        self.affine_output = nn.Linear(in_features=args[\"layers\"][-1] + self.factor_num_mf, out_features=1)\n","        self.logistic = nn.Sigmoid()\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n","        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n","        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n","        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n","        \n","        for m in self.fc_layers:\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                \n","        nn.init.xavier_uniform_(self.affine_output.weight)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n","        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n","\n","        user_embedding_mf = self.embedding_user_mf(user_indices)\n","        item_embedding_mf = self.embedding_item_mf(item_indices)\n","\n","        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n","        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n","\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            mlp_vector = self.fc_layers[idx](mlp_vector)\n","\n","        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating.squeeze()"]},{"cell_type":"markdown","metadata":{},"source":["Pytorch Config"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T06:24:15.070397Z","iopub.status.busy":"2024-01-27T06:24:15.069916Z","iopub.status.idle":"2024-01-27T06:24:15.083142Z","shell.execute_reply":"2024-01-27T06:24:15.081890Z","shell.execute_reply.started":"2024-01-27T06:24:15.070353Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import numpy as np \n","import torch\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"markdown","metadata":{},"source":["Evaluation Metrics"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T06:24:15.086598Z","iopub.status.busy":"2024-01-27T06:24:15.086128Z","iopub.status.idle":"2024-01-27T06:24:15.105176Z","shell.execute_reply":"2024-01-27T06:24:15.103991Z","shell.execute_reply.started":"2024-01-27T06:24:15.086555Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","\n","def RMSE(predictions, ground_truth_reate):\n","    squared_diff = (predictions - ground_truth_reate) ** 2\n","    mean_squared_diff = np.mean(squared_diff)\n","    return np.sqrt(mean_squared_diff)\n","\n","def dcg_at_k(r, k):\n","    r = np.asfarray(r)[:k]\n","    if r.size:\n","        return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n","    return 0.\n","\n","def ndcg_at_k(r, ideal_rate_set, k):\n","    dcg_max = dcg_at_k(ideal_rate_set, k)\n","    if not dcg_max:\n","        return 0.\n","    return dcg_at_k(r, k) / dcg_max\n","\n","def correlation_rank(predictions):\n","    _, indices = torch.topk(predictions, len(predictions))\n","    squareDifferences = np.array([pow(i - indices[i], 2) for i in range(len(indices))])\n","    return 1 - ((6 * np.sum(squareDifferences)) / (len(predictions) * (pow(len(predictions), 2) - 1)))\n","\n","def precision_at_10(indices, ground_truth_reate):\n","    relevance = [index for index in indices if ground_truth_reate[index] >= 2]\n","    return len(relevance)/len(indices)\n","    \n","\n","def evaluate(model, data, top_k, device):\n","    NDCG, rank_correlation, p_10, rmse = [], [], [], []\n","    unique_user_ids = data['User_ID'].unique()\n","    \n","    for user_id in unique_user_ids:\n","        # get data of user include [\"User_ID\", \"Group_ID\", \"Interest_Rate\"]\n","        ground_truth = (data[data['User_ID'] == user_id]).sort_values(by='Interest_Rate', ascending=False)\n","        \n","        # creating tensor for items that user ahs given rate them and same as its size create tensor for user\n","        itemsTensor = torch.tensor(np.array(ground_truth[\"Group_ID\"]))\n","        userTensor = torch.full((len(ground_truth[\"Group_ID\"]),), user_id)\n","        \n","        itemsTensor = itemsTensor.to(device)\n","        userTensor = userTensor.to(device)\n","        \n","        predictions = model(userTensor, itemsTensor)\n","        _, indices = torch.topk(predictions, top_k)\n","        rate_recommends = torch.take(torch.tensor(np.array(ground_truth[\"Interest_Rate\"])), indices).cpu().numpy().tolist()\n","        \n","        rank_correlation.append(correlation_rank(predictions))\n","        \n","        rmse.append(RMSE(predictions.detach().numpy(), np.array(ground_truth[\"Interest_Rate\"])))\n","        \n","        p_10.append(precision_at_10(indices, np.array(ground_truth[\"Interest_Rate\"])))\n","        \n","        NDCG.append(ndcg_at_k(rate_recommends, np.array(ground_truth[\"Interest_Rate\"]), 10))\n","    \n","    return np.mean(NDCG), np.mean(rank_correlation), np.mean(p_10), np.mean(rmse)"]},{"cell_type":"markdown","metadata":{},"source":["Main Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T06:24:15.107593Z","iopub.status.busy":"2024-01-27T06:24:15.107120Z"},"trusted":true},"outputs":[],"source":["import os\n","import time\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from tensorboardX import SummaryWriter\n","\n","\n","# set device and parameters\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","writer = SummaryWriter()\n","args = {\n","    \"seed\": 42,\n","    \"lr\": 0.001,\n","    \"dropout\": 0.2,\n","    \"batch_size\": 102400,\n","    \"epochs\": 10,\n","    \"top_k\": 10,\n","    \"factor_num\": 64,\n","    \"layers\": [64,32,16,8],\n","    \"num_batchSize_test_valid\": 100,\n","    \"out\": True\n","}\n","# seed for Reproducibility\n","seed_everything(args[\"seed\"])\n","\n","# load data\n","ml_1m = pd.read_csv('/kaggle/input/smart-postchin-data/Smart_Postchin_Data.csv')\n","\n","# set the num_users, items\n","num_users = ml_1m['User_ID'].nunique()+1\n","num_items = ml_1m['Group_ID'].nunique()+1\n","\n","# construct the train and test datasets\n","data = NFC_Data(args, ml_1m)\n","\n","train_loader =data.get_train_instance()\n","validation_loader = data.get_validation_instance()\n","test_loader =data.get_test_instance()\n","\n","\n","# set model and loss, optimizer\n","model = NeuMF(args, num_users, num_items)\n","model = model.to(device)\n","loss_function = RMSELoss()  # Using the custom RMSE loss function\n","optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n","\n","# train, evaluation\n","best_hr = 0\n","for epoch in range(1, args[\"epochs\"]+1):\n","\tmodel.train()\n","\tstart_time = time.time()\n","\n","\tfor user, item, label in train_loader:\n","\t\tuser = user.to(device)\n","\t\titem = item.to(device)\n","\t\tlabel = label.to(device)\n","        \n","\t\toptimizer.zero_grad()\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label)\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\twriter.add_scalar('loss/Train_loss', loss.item(), epoch)\n","    \n","\tmodel.eval()\n","\tNDCG, rank_correlation, p_10, rmse = evaluate(model, validation_loader, args[\"top_k\"], device)\n","\n","\telapsed_time = time.time() - start_time\n","\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n","\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n","\tprint(\"NDCG: {:.3f}\\trank_correlation: {:.3f}\\tP@10: {:.3f}\\trmse: {:.3f}\".format(NDCG, rank_correlation, p_10, rmse))\n","\n","writer.close()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4340651,"sourceId":7457029,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
